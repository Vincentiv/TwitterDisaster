{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import torch\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(input_str, search_str):\n",
    "    l1 = []\n",
    "    length = len(input_str)\n",
    "    index = 0\n",
    "    while index < length:\n",
    "        i = input_str.find(search_str, index)\n",
    "        if i == -1:\n",
    "            return l1\n",
    "        l1.append(i)\n",
    "        index = i + 1\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data\n",
    "def do_qa_train(train):\n",
    "    output = {}\n",
    "    output['version'] = 'v1.0'\n",
    "    output['data'] = []\n",
    "    \n",
    "    for line in train:\n",
    "        paragraphs = []\n",
    "        \n",
    "        context = line[1]\n",
    "        \n",
    "        qas = []\n",
    "        question = line[-1]\n",
    "        qid = line[0]\n",
    "        answers = []\n",
    "        answer = line[2]\n",
    "        if type(answer) != str or type(context) != str or type(question) != str:\n",
    "            print(context, type(context))\n",
    "            print(answer, type(answer))\n",
    "            print(question, type(question))\n",
    "            continue\n",
    "        answer_starts = find_all(context, answer)\n",
    "        for answer_start in answer_starts:\n",
    "            answers.append({'answer_start': answer_start, 'text': answer})\n",
    "        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n",
    "        \n",
    "        paragraphs.append({'context': context, 'qas': qas})\n",
    "        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n",
    "        \n",
    "    return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data\n",
    "\n",
    "def do_qa_test(test):\n",
    "    output = {}\n",
    "    output['version'] = 'v1.0'\n",
    "    output['data'] = []\n",
    "    \n",
    "    for line in test:\n",
    "        paragraphs = []\n",
    "       \n",
    "        context = line[1]\n",
    "       \n",
    "        qas = []\n",
    "        question = line[-1]\n",
    "        qid = line[0]\n",
    "        if type(context) != str or type(question) != str:\n",
    "            print(context, type(context))\n",
    "            print(answer, type(answer))\n",
    "            print(question, type(question))\n",
    "            continue\n",
    "        answers = []\n",
    "        answers.append({'answer_start': 1000000, 'text': '__None__'})\n",
    "        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n",
    "       \n",
    "        paragraphs.append({'context': context, 'qas': qas})\n",
    "        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan <class 'float'>\n",
      "nan <class 'float'>\n",
      "neutral <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_roberta = pd.read_csv(\"input/train.csv\")\n",
    "df_test_roberta = pd.read_csv(\"input/test.csv\")\n",
    "df_submission_roberta=pd.read_csv(\"input/submission.csv\")\n",
    "\n",
    "\n",
    "train_roberta = np.array(df_train_roberta)\n",
    "test_roberta = np.array(df_test_roberta)\n",
    "\n",
    "y=df_train_roberta.selected_text\n",
    "df_train_roberta.drop(columns=['selected_text'], inplace= True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df_train_roberta, y,train_size=0.8, test_size=0.2,random_state=1)\n",
    "\n",
    "# print(df_train_distillbert.shape)\n",
    "# print(X_train)\n",
    "# print(X_valid)\n",
    "# print(y_train)\n",
    "# print(y_valid)\n",
    "\n",
    "\n",
    "train=pd.concat([X_train, y_train], axis=1)\n",
    "valid=pd.concat([X_valid, y_valid], axis=1)\n",
    "\n",
    "train_df=train.reindex(columns=['textID','text','selected_text','sentiment']).reset_index(drop=True)\n",
    "valid_df=valid.reindex(columns=['textID','text','selected_text','sentiment']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train = np.array(train_df)\n",
    "valid = np.array(valid_df)\n",
    "test = np.array(test_roberta)\n",
    "\n",
    "qa_train = do_qa_train(train)\n",
    "qa_valid = do_qa_train(valid)\n",
    "qa_test = do_qa_test(test)\n",
    "\n",
    "print(qa_train)\n",
    "print(valid.shape)\n",
    "\n",
    "\n",
    "with open('data/train.json', 'w') as outfile:\n",
    "    json.dump(qa_train, outfile)\n",
    "    \n",
    "with open('data/test.json', 'w') as outfile:\n",
    "    json.dump(qa_test, outfile)\n",
    "\n",
    "with open('data/valid.json', 'w') as outfile:\n",
    "    json.dump(qa_valid, outfile)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:\\Users\\Vincent\\PycharmProjects\\ML\\transformers\\examples\\run_squad.py \\\n",
    "--model_type roberta \\\n",
    "--model_name_or_path roberta-large \\\n",
    "--do_lower_case \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--cache_dir cache \\\n",
    "--train_file data/train.json \\\n",
    "--predict_file data/test.json \\\n",
    "--learning_rate 5e-5 \\\n",
    "--num_train_epochs 1 \\\n",
    "--max_seq_length 128 \\\n",
    "--doc_stride 64 \\\n",
    "--output_dir results_roberta_large \\\n",
    "--per_gpu_eval_batch_size=8 \\\n",
    "--per_gpu_train_batch_size=8\\\n",
    "--gradient_accumulation_steps=2\\\n",
    "\n",
    "# --save_steps=200000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce GTX 1070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "#         print('debut')\n",
    "#         print(obj)\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "            print('là')\n",
    "    except:\n",
    "        pass\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1070'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n",
      "__pyTorch VERSION: 1.4.0\n",
      "__CUDA VERSION\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:12:52_Pacific_Daylight_Time_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "__CUDNN VERSION: 7501\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 device(s) found.\n",
      "0 GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pycuda\n",
    "from pycuda import compiler\n",
    "import pycuda.driver as drv\n",
    "\n",
    "drv.init()\n",
    "print(\"%d device(s) found.\" % drv.Device.count())\n",
    "           \n",
    "for ordinal in range(drv.Device.count()):\n",
    "    dev = drv.Device(ordinal)\n",
    "    print (ordinal, dev.name())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
